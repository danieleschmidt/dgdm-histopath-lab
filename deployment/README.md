# DGDM Histopath Lab - Production Deployment Guide

*Generated by Terry for Terragon Labs SDLC v4.0*

## ðŸš€ Quick Start

### Prerequisites

- Kubernetes cluster (v1.21+)
- Docker registry access
- kubectl configured
- Helm (optional but recommended)

### 1. Build and Push Docker Images

```bash
# Build main application image
docker build -t your-registry/dgdm-histopath:latest -f deployment/Dockerfile .

# Build worker image
docker build -t your-registry/dgdm-histopath-worker:latest --target worker -f deployment/Dockerfile .

# Build scheduler image
docker build -t your-registry/dgdm-histopath-scheduler:latest --target scheduler -f deployment/Dockerfile .

# Push images
docker push your-registry/dgdm-histopath:latest
docker push your-registry/dgdm-histopath-worker:latest
docker push your-registry/dgdm-histopath-scheduler:latest
```

### 2. Create Namespace and Secrets

```bash
# Create namespace
kubectl create namespace dgdm-histopath

# Create secrets
kubectl create secret generic dgdm-secrets \
  --from-literal=database-host=your-db-host \
  --from-literal=database-password=your-db-password \
  --from-literal=redis-host=your-redis-host \
  --from-literal=redis-password=your-redis-password \
  -n dgdm-histopath
```

### 3. Deploy Application

```bash
# Deploy main application
kubectl apply -f deployment/production_config.yaml

# Deploy monitoring (optional)
kubectl apply -f deployment/monitoring.yaml
```

## ðŸ“Š Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Load Balancer â”‚â”€â”€â”€â”€â”‚   API Gateway   â”‚â”€â”€â”€â”€â”‚   Prometheus    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚                        â”‚
         â–¼                        â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   API Pods      â”‚    â”‚  Worker Pods    â”‚    â”‚    Grafana      â”‚
â”‚  (3 replicas)   â”‚    â”‚  (5 replicas)   â”‚    â”‚   Dashboard     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚                        
         â–¼                        â–¼                        
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                
â”‚   PostgreSQL    â”‚    â”‚     Redis       â”‚                
â”‚   Database      â”‚    â”‚     Cache       â”‚                
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                
```

## ðŸ”§ Configuration

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `ENVIRONMENT` | Deployment environment | `production` |
| `LOG_LEVEL` | Logging level | `INFO` |
| `MAX_CONCURRENT_TASKS` | Max concurrent quantum tasks | `32` |
| `WORKER_PROCESSES` | Number of worker processes | `8` |
| `CACHE_SIZE_MB` | Cache size in MB | `2000` |
| `DB_HOST` | Database host | - |
| `DB_PASSWORD` | Database password | - |
| `REDIS_HOST` | Redis host | - |
| `REDIS_PASSWORD` | Redis password | - |

### Resource Requirements

#### API Pods
- **CPU**: 1-2 cores
- **Memory**: 2-4 GB
- **Storage**: 10 GB (logs, cache)

#### Worker Pods
- **CPU**: 2-4 cores
- **Memory**: 4-8 GB
- **GPU**: 1x NVIDIA GPU (optional)
- **Storage**: 50 GB (models, data processing)

## ðŸŽ›ï¸ Scaling Configuration

### Horizontal Pod Autoscaler

The application automatically scales based on:
- **CPU utilization**: Target 70%
- **Memory utilization**: Target 80%
- **Custom metrics**: Queue depth, response time

```yaml
# Scale up policy: 50% increase every 60s
# Scale down policy: 25% decrease every 60s
minReplicas: 2
maxReplicas: 20
```

### Cluster Autoscaler

Ensure your Kubernetes cluster has cluster autoscaler configured for GPU nodes:

```yaml
nodeGroups:
  - name: gpu-workers
    minSize: 0
    maxSize: 10
    instanceType: g4dn.xlarge  # or equivalent
```

## ðŸ“ˆ Monitoring & Observability

### Metrics Endpoints

- **Application metrics**: `http://api:9090/metrics`
- **Health check**: `http://api:8080/health`
- **Readiness check**: `http://api:8080/ready`

### Key Metrics

- `dgdm_quantum_tasks_total` - Total quantum tasks processed
- `dgdm_quantum_queue_depth` - Current queue depth
- `dgdm_model_inference_duration` - Model inference latency
- `dgdm_gpu_utilization` - GPU utilization percentage
- `dgdm_cache_hit_ratio` - Cache hit ratio

### Alerts

Critical alerts are configured for:
- API availability (> 1 minute downtime)
- High error rate (> 5% for 2 minutes)
- High latency (> 2 seconds 95th percentile)
- Resource exhaustion

## ðŸ”’ Security

### Network Security

- **Network policies** restrict inter-pod communication
- **TLS termination** at load balancer
- **Ingress filtering** with WAF rules

### Data Security

- **Encryption at rest** for sensitive data
- **PHI detection** and redaction
- **Audit logging** for all API requests
- **Role-based access control** (RBAC)

### Container Security

- **Non-root user** (uid: 1000)
- **Read-only root filesystem**
- **Security contexts** with restricted capabilities
- **Image vulnerability scanning**

## ðŸš¨ Troubleshooting

### Common Issues

#### 1. Pods Failing to Start

```bash
# Check pod status
kubectl get pods -n dgdm-histopath

# Check pod logs
kubectl logs -f deployment/dgdm-histopath-api -n dgdm-histopath

# Check events
kubectl get events -n dgdm-histopath --sort-by='.lastTimestamp'
```

#### 2. Database Connection Issues

```bash
# Test database connectivity
kubectl exec -it deployment/dgdm-histopath-api -n dgdm-histopath -- \
  python -c "
import os, psycopg2
conn = psycopg2.connect(
    host=os.getenv('DB_HOST'),
    database=os.getenv('DB_NAME'),
    user=os.getenv('DB_USER'),
    password=os.getenv('DB_PASSWORD')
)
print('Database connection successful')
"
```

#### 3. High Memory Usage

```bash
# Check memory metrics
kubectl top pods -n dgdm-histopath

# Restart high-memory pods
kubectl rollout restart deployment/dgdm-histopath-api -n dgdm-histopath
```

#### 4. GPU Issues

```bash
# Check GPU availability
kubectl describe nodes -l node-type=gpu

# Check GPU usage in pods
kubectl exec -it deployment/dgdm-histopath-worker -n dgdm-histopath -- nvidia-smi
```

### Health Check Commands

```bash
# Overall application health
curl -f http://your-domain/health

# Detailed health check
kubectl exec -it deployment/dgdm-histopath-api -n dgdm-histopath -- \
  python /app/healthcheck.py
```

## ðŸ”„ CI/CD Integration

### GitHub Actions Integration

```yaml
# Add to .github/workflows/deploy.yml
- name: Deploy to Production
  run: |
    kubectl apply -f deployment/production_config.yaml
    kubectl rollout status deployment/dgdm-histopath-api -n dgdm-histopath
```

### Blue-Green Deployment

```bash
# Create blue deployment
kubectl apply -f deployment/production_config.yaml

# Switch traffic
kubectl patch service dgdm-histopath-api-service -n dgdm-histopath \
  -p '{"spec":{"selector":{"version":"green"}}}'
```

## ðŸ“š Advanced Configuration

### Custom Resource Limits

```yaml
resources:
  requests:
    memory: "4Gi"
    cpu: "2000m"
    nvidia.com/gpu: 1
  limits:
    memory: "8Gi"
    cpu: "4000m"
    nvidia.com/gpu: 1
```

### Persistent Volume Configuration

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: dgdm-model-cache
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: fast-ssd
  resources:
    requests:
      storage: 100Gi
```

### Service Mesh Integration

For production environments with service mesh (Istio, Linkerd):

```yaml
apiVersion: v1
kind: Service
metadata:
  name: dgdm-histopath-api-service
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: nlb
    linkerd.io/inject: enabled
```

## ðŸ“ž Support

For production issues:

1. Check application logs: `kubectl logs -f deployment/dgdm-histopath-api -n dgdm-histopath`
2. Review Grafana dashboards: `http://your-grafana-domain/d/dgdm-overview`
3. Check Prometheus alerts: `http://your-prometheus-domain/alerts`
4. Run health checks: `python /app/healthcheck.py`

---

*This deployment guide ensures production-ready operation with high availability, scalability, and comprehensive monitoring for the DGDM Histopath Lab platform.*